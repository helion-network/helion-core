version: "3.9"

services:
  helion-test-server:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: helion-test-server
    # restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      # Comma-separated model IDs to load (edit as needed). Must not be empty.
      MODELS: "meta-llama/Llama-3.2-1B-Instruct,openai/gpt-oss-20b"
      # Default initial peer for public swarm; override for private swarms
      INITIAL_PEERS: "/ip4/34.143.175.170/tcp/31337/p2p/QmPXGhXJRDZZLRPDKSRXGnM9FiccEncxqrLjWSLZhjgKZS"
      # Optional: HF token for gated/private models
      HF_TOKEN: ""
      # Optional shared DHT prefix
      DHT_PREFIX: ""
      PORT: "8080"
    command:
      - bash
      - -lc
      - >
        pip install --no-cache-dir uvicorn fastapi &&
        python scripts/serve_multi_models.py
        --models "$${MODELS:-meta-llama/Llama-3.2-1B-Instruct,openai/gpt-oss-20b}"
        --initial-peers "$${INITIAL_PEERS:-/ip4/34.143.175.170/tcp/31337/p2p/QmPXGhXJRDZZLRPDKSRXGnM9FiccEncxqrLjWSLZhjgKZS}"
        --hf-token "${HF_TOKEN}"
        --dht-prefix "$${DHT_PREFIX:-}"
        --port "$${PORT:-8080}"

